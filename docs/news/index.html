<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>All news • sparklyr</title>

<!-- jquery -->
<script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script>
<!-- Bootstrap -->
<link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">

<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

<!-- Font Awesome icons -->
<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">


<!-- pkgdown -->
<link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script>
<script src="../pkgdown.js"></script>
  
  
<!-- mathjax -->
<script src='https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->


  </head>

  <body>
    <div class="container template-news">
      <header>
      <div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">sparklyr</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../index.html">Home</a>
</li>
<li>
  <a href="../articles/gallery.html">Gallery</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Guides
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="../articles/guides-dplyr.html">dplyr</a>
    </li>
    <li>
      <a href="../articles/guides-mllib.html">mllib</a>
    </li>
    <li>
      <a href="../articles/guides-distributed-r.html">Distributed R</a>
    </li>
    <li>
      <a href="../articles/guides-extensions.html">Extensions</a>
    </li>
    <li>
      <a href="../articles/guides-caching.html">Caching</a>
    </li>
    <li>
      <a href="../articles/guides-h2o.html">H2O</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Deployment
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="../articles/deployment-overview.html">Overview</a>
    </li>
    <li>
      <a href="../articles/deployment-data-lakes.html">Data Lakes</a>
    </li>
    <li>
      <a href="../articles/deployment-cdh.html">Cloudera</a>
    </li>
    <li>
      <a href="../articles/deployment-amazon.html">Amazon</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Reference
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="../reference/index.html">Functions</a>
    </li>
    <li>
      <a href="https://github.com/rstudio/cheatsheets/raw/master/source/pdfs/sparklyr-cheatsheet.pdf">Cheatsheet</a>
    </li>
    <li>
      <a href="../articles/reference-media.html">Media</a>
    </li>
    <li>
      <a href="../news/index.html">News</a>
    </li>
  </ul>
</li>
      </ul>
      
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/rstudio/sparklyr">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      
      </header>

      <div class="row">

  <div class="col-md-9">
    <div class="page-header">
      <h1>Change log <small>All releases</small></h1>
    </div>

    <div class="contents">
    <div id="sparklyr-0-7-unreleased" class="section level1">
<h1 class="hasAnchor">
<a href="#sparklyr-0-7-unreleased" class="anchor"></a>Sparklyr 0.7 (UNRELEASED)</h1>
<ul>
<li><p>Avoid tasks failing under <code><a href="../reference/spark_apply.html">spark_apply()</a></code> and multiple concurrent partitions running while selecting backend port.</p></li>
<li><p>Added support for numeric arguments for <code>n</code> in <code>lead()</code> for dplyr.</p></li>
<li><p>Added unsupported error message to <code>sample_n()</code> and <code>sample_frac()</code> when Spark is not 2.0 or higher.</p></li>
<li><p>Fixed <code>SIGPIPE</code> error under <code><a href="../reference/spark-connections.html">spark_connect()</a></code> immediately after a <code><a href="../reference/spark-connections.html">spark_disconnect()</a></code> operation.</p></li>
<li><p>Added support for <code>sparklyr.apply.env.</code> under <code><a href="../reference/spark_config.html">spark_config()</a></code> to allow <code><a href="../reference/spark_apply.html">spark_apply()</a></code> to initializae environment varaibles.</p></li>
<li><p>Added support for <code><a href="../reference/spark_read_text.html">spark_read_text()</a></code> and <code><a href="../reference/spark_write_text.html">spark_write_text()</a></code> to read from and to plain text files.</p></li>
<li><p>Addesd support for RStudio project templates to create an “R Package using sparklyr”.</p></li>
<li><p>Fix <code>compute()</code> to trigger refresh of the connections view.</p></li>
<li><p>Added a <code>k</code> argument to <code><a href="../reference/ml_pca.html">ml_pca()</a></code> to enable specification of number of principal components to extract. Also implemented <code><a href="../reference/sdf_project.html">sdf_project()</a></code> to project datasets using the results of <code><a href="../reference/ml_pca.html">ml_pca()</a></code> models.</p></li>
<li><p>Added to <code><a href="../reference/ml_random_forest.html">ml_random_forest()</a></code> the following hyperparameter arguments: <code>min.info.gain</code>, <code>col.sample.rate</code>, <code>min.rows</code>, <code>impurity</code>, and <code>thresholds</code>. Also added <code>seed</code> for reproducibility.</p></li>
</ul>
</div>
    <div id="sparklyr-0-6-1-unreleased" class="section level1">
<h1 class="hasAnchor">
<a href="#sparklyr-0-6-1-unreleased" class="anchor"></a>Sparklyr 0.6.1 (UNRELEASED)</h1>
<ul>
<li><p>Fixed error in <code><a href="../reference/spark_apply.html">spark_apply()</a></code> that may triggered when multiple CPUs are used in a single node due to race conditions while accesing the gateway service and another in the <code>JVMObjectTracker</code>.</p></li>
<li><p><code><a href="../reference/spark_apply.html">spark_apply()</a></code> now supports explicit column types using the <code>columns</code> argument to avoid sampling types.</p></li>
<li><p><code><a href="../reference/spark_apply.html">spark_apply()</a></code> with <code>group_by</code> no longer requires persisting to disk nor memory.</p></li>
<li><p>Added support for Spark 1.6.3 under <code><a href="../reference/spark_install.html">spark_install()</a></code>.</p></li>
<li><p>Added support for Spark 1.6.3 under <code><a href="../reference/spark_install.html">spark_install()</a></code></p></li>
<li><p><code><a href="../reference/spark_apply.html">spark_apply()</a></code> now logs the current callstack when it fails.</p></li>
<li><p>Fixed error triggered while processing empty partitions in <code><a href="../reference/spark_apply.html">spark_apply()</a></code>.</p></li>
<li><p>Fixed slow printing issue caused by <code>print</code> calculating the total row count, which is expensive for some tables.</p></li>
<li><p>Fixed <code>sparklyr 0.6</code> issue blocking concurrent <code>sparklyr</code> connections, which required to set <code>config$sparklyr.gateway.remote = FALSE</code> as workaround.</p></li>
</ul>
</div>
    <div id="sparklyr-0-6-0" class="section level1">
<h1 class="hasAnchor">
<a href="#sparklyr-0-6-0" class="anchor"></a>Sparklyr 0.6.0</h1>
<div id="distributed-r" class="section level3">
<h3 class="hasAnchor">
<a href="#distributed-r" class="anchor"></a>Distributed R</h3>
<ul>
<li><p>Added <code>packages</code> parameter to <code><a href="../reference/spark_apply.html">spark_apply()</a></code> to distribute packages across worker nodes automatically.</p></li>
<li><p>Added <code>sparklyr.closures.rlang</code> as a <code><a href="../reference/spark_config.html">spark_config()</a></code> value to support generic closures provided by the <code>rlang</code> package.</p></li>
<li><p>Added config options <code>sparklyr.worker.gateway.address</code> and <code>sparklyr.worker.gateway.port</code> to configure gateway used under worker nodes.</p></li>
<li><p>Added <code>group_by</code> parameter to <code><a href="../reference/spark_apply.html">spark_apply()</a></code>, to support operations over groups of dataframes.</p></li>
<li><p>Added <code><a href="../reference/spark_apply.html">spark_apply()</a></code>, allowing users to use R code to directly manipulate and transform Spark DataFrames.</p></li>
</ul>
</div>
<div id="external-data" class="section level3">
<h3 class="hasAnchor">
<a href="#external-data" class="anchor"></a>External Data</h3>
<ul>
<li><p>Added <code><a href="../reference/spark_write_source.html">spark_write_source()</a></code>. This function writes data into a Spark data source which can be loaded through an Spark package.</p></li>
<li><p>Added <code><a href="../reference/spark_write_jdbc.html">spark_write_jdbc()</a></code>. This function writes from a Spark DataFrame into a JDBC connection.</p></li>
<li><p>Added <code>columns</code> parameter to <code>spark_read_*()</code> functions to load data with named columns or explicit column types.</p></li>
<li><p>Added <code>partition_by</code> parameter to <code><a href="../reference/spark_write_csv.html">spark_write_csv()</a></code>, <code><a href="../reference/spark_write_json.html">spark_write_json()</a></code>, <code><a href="../reference/spark_write_table.html">spark_write_table()</a></code> and <code><a href="../reference/spark_write_parquet.html">spark_write_parquet()</a></code>.</p></li>
<li><p>Added <code><a href="../reference/spark_read_source.html">spark_read_source()</a></code>. This function reads data from a Spark data source which can be loaded through an Spark package.</p></li>
<li><p>Added support for <code>mode = "overwrite"</code> and <code>mode = "append"</code> to <code><a href="../reference/spark_write_csv.html">spark_write_csv()</a></code>.</p></li>
<li><p><code><a href="../reference/spark_write_table.html">spark_write_table()</a></code> now supports saving to default Hive path.</p></li>
<li><p>Improved performance of <code><a href="../reference/spark_read_csv.html">spark_read_csv()</a></code> reading remote data when <code>infer_schema = FALSE</code>.</p></li>
<li><p>Added <code><a href="../reference/spark_read_jdbc.html">spark_read_jdbc()</a></code>. This function reads from a JDBC connection into a Spark DataFrame.</p></li>
<li><p>Renamed <code><a href="../reference/spark_load_table.html">spark_load_table()</a></code> and <code><a href="../reference/spark_save_table.html">spark_save_table()</a></code> into <code><a href="../reference/spark_read_table.html">spark_read_table()</a></code> and <code><a href="../reference/spark_write_table.html">spark_write_table()</a></code> for consistency with existing <code>spark_read_*()</code> and <code>spark_write_*()</code> functions.</p></li>
<li><p>Added support to specify a vector of column names in <code><a href="../reference/spark_read_csv.html">spark_read_csv()</a></code> to specify column names without having to set the type of each column.</p></li>
<li><p>Improved <code>copy_to()</code>, <code>sdf_copy_to()</code> and <code>dbWriteTable()</code> performance under <code>yarn-client</code> mode.</p></li>
</ul>
</div>
<div id="dplyr" class="section level3">
<h3 class="hasAnchor">
<a href="#dplyr" class="anchor"></a>dplyr</h3>
<ul>
<li><p>Support for <code>cumprod()</code> to calculate cumulative products.</p></li>
<li><p>Support for <code>cor()</code>, <code>cov()</code>, <code>sd()</code> and <code>var()</code> as window functions.</p></li>
<li><p>Support for Hive built-in operators <code>%like%</code>, <code>%rlike%</code>, and <code>%regexp%</code> for matching regular expressions in <code>filter()</code> and <code>mutate()</code>.</p></li>
<li><p>Support for dplyr (&gt;= 0.6) which among many improvements, increases performance in some queries by making use of a new query optimizer.</p></li>
<li><p><code>sample_frac()</code> takes a fraction instead of a percent to match dplyr.</p></li>
<li><p>Improved performance of <code>sample_n()</code> and <code>sample_frac()</code> through the use of <code>TABLESAMPLE</code> in the generated query.</p></li>
</ul>
</div>
<div id="databases" class="section level3">
<h3 class="hasAnchor">
<a href="#databases" class="anchor"></a>Databases</h3>
<ul>
<li><p>Added <code><a href="../reference/src_databases.html">src_databases()</a></code>. This function list all the available databases.</p></li>
<li><p>Added <code><a href="../reference/tbl_change_db.html">tbl_change_db()</a></code>. This function changes current database.</p></li>
</ul>
</div>
<div id="dataframes" class="section level3">
<h3 class="hasAnchor">
<a href="#dataframes" class="anchor"></a>DataFrames</h3>
<ul>
<li><p>Added <code><a href="../reference/sdf_len.html">sdf_len()</a></code>, <code><a href="../reference/sdf_seq.html">sdf_seq()</a></code> and <code><a href="../reference/sdf_along.html">sdf_along()</a></code> to help generate numeric sequences as Spark DataFrames.</p></li>
<li><p>Added <code><a href="../reference/checkpoint_directory.html">spark_set_checkpoint_dir()</a></code>, <code><a href="../reference/checkpoint_directory.html">spark_get_checkpoint_dir()</a></code>, and <code><a href="../reference/sdf_checkpoint.html">sdf_checkpoint()</a></code> to enable checkpointing.</p></li>
<li><p>Added <code><a href="../reference/sdf_broadcast.html">sdf_broadcast()</a></code> which can be used to hint the query optimizer to perform a broadcast join in cases where a shuffle hash join is planned but not optimal.</p></li>
<li><p>Added <code><a href="../reference/sdf_repartition.html">sdf_repartition()</a></code>, <code><a href="../reference/sdf_coalesce.html">sdf_coalesce()</a></code>, and <code><a href="../reference/sdf_num_partitions.html">sdf_num_partitions()</a></code> to support repartitioning and getting the number of partitions of Spark DataFrames.</p></li>
<li><p>Added <code><a href="../reference/sdf_bind.html">sdf_bind_rows()</a></code> and <code><a href="../reference/sdf_bind.html">sdf_bind_cols()</a></code> – these functions are the <code>sparklyr</code> equivalent of <code><a href="http://dplyr.tidyverse.org/reference/bind.html">dplyr::bind_rows()</a></code> and <code><a href="http://dplyr.tidyverse.org/reference/bind.html">dplyr::bind_cols()</a></code>.</p></li>
<li><p>Added <code><a href="../reference/sdf_separate_column.html">sdf_separate_column()</a></code> – this function allows one to separate components of an array / vector column into separate scalar-valued columns.</p></li>
<li><p><code><a href="../reference/sdf_with_sequential_id.html">sdf_with_sequential_id()</a></code> now supports <code>from</code> parameter to choose the starting value of the id column.</p></li>
<li><p>Added <code><a href="../reference/sdf_pivot.html">sdf_pivot()</a></code>. This function provides a mechanism for constructing pivot tables, using Spark’s ‘groupBy’ + ‘pivot’ functionality, with a formula interface similar to that of <code><a href="http://www.rdocumentation.org/packages/reshape2/topics/cast">reshape2::dcast()</a></code>.</p></li>
</ul>
</div>
<div id="mllib" class="section level3">
<h3 class="hasAnchor">
<a href="#mllib" class="anchor"></a>MLlib</h3>
<ul>
<li><p>Added <code>vocabulary.only</code> to <code><a href="../reference/ft_count_vectorizer.html">ft_count_vectorizer()</a></code> to retrieve the vocabulary with ease.</p></li>
<li><p>GLM type models now support <code>weights.column</code> to specify weights in model fitting. (#217)</p></li>
<li><p><code><a href="../reference/ml_logistic_regression.html">ml_logistic_regression()</a></code> now supports multinomial regression, in addition to binomial regression [requires Spark 2.1.0 or greater]. (#748)</p></li>
<li><p>Implemented <code>residuals()</code> and <code><a href="../reference/sdf_residuals.html">sdf_residuals()</a></code> for Spark linear regression and GLM models. The former returns a R vector while the latter returns a <code>tbl_spark</code> of training data with a <code>residuals</code> column added.</p></li>
<li><p>Added <code><a href="../reference/ml_model_data.html">ml_model_data()</a></code>, used for extracting data associated with Spark ML models.</p></li>
<li><p>The <code><a href="../reference/ml_saveload.html">ml_save()</a></code> and <code><a href="../reference/ml_saveload.html">ml_load()</a></code> functions gain a <code>meta</code> argument, allowing users to specify where R-level model metadata should be saved independently of the Spark model itself. This should help facilitate the saving and loading of Spark models used in non-local connection scenarios.</p></li>
<li><p><code><a href="../reference/ml_als_factorization.html">ml_als_factorization()</a></code> now supports the implicit matrix factorization and nonnegative least square options.</p></li>
<li><p>Added <code><a href="../reference/ft_count_vectorizer.html">ft_count_vectorizer()</a></code>. This function can be used to transform columns of a Spark DataFrame so that they might be used as input to <code><a href="../reference/ml_lda.html">ml_lda()</a></code>. This should make it easier to invoke <code><a href="../reference/ml_lda.html">ml_lda()</a></code> on Spark data sets.</p></li>
</ul>
</div>
<div id="broom" class="section level3">
<h3 class="hasAnchor">
<a href="#broom" class="anchor"></a>Broom</h3>
<ul>
<li>Implemented <code><a href="http://www.rdocumentation.org/packages/broom/topics/tidy">tidy()</a></code>, <code><a href="http://www.rdocumentation.org/packages/broom/topics/augment">augment()</a></code>, and <code><a href="http://www.rdocumentation.org/packages/broom/topics/glance">glance()</a></code> from tidyverse/broom for <code>ml_model_generalized_linear_regression</code> and <code>ml_model_linear_regression</code> models.</li>
</ul>
</div>
<div id="r-compatibility" class="section level3">
<h3 class="hasAnchor">
<a href="#r-compatibility" class="anchor"></a>R Compatibility</h3>
<ul>
<li>Implemented <code>cbind.tbl_spark()</code>. This method works by first generating index columns using <code><a href="../reference/sdf_with_sequential_id.html">sdf_with_sequential_id()</a></code> then performing <code>inner_join()</code>. Note that dplyr <code>_join()</code> functions should still be used for DataFrames with common keys since they are less expensive.</li>
</ul>
</div>
<div id="connections" class="section level3">
<h3 class="hasAnchor">
<a href="#connections" class="anchor"></a>Connections</h3>
<ul>
<li><p>Increased default number of concurrent connections by setting default for <code>spark.port.maxRetries</code> from 16 to 128.</p></li>
<li><p>Support for gateway connections <code>sparklyr://hostname:port/session</code> and using <code>spark-submit --class sparklyr.Shell sparklyr-2.1-2.11.jar &lt;port&gt; &lt;id&gt; --remote</code>.</p></li>
<li><p>Added support for <code>sparklyr.gateway.service</code> and <code>sparklyr.gateway.remote</code> to enable/disable the gateway in service and to accept remote connections required for Yarn Cluster mode.</p></li>
<li><p>Added support for Yarn Cluster mode using <code>master = "yarn-cluster"</code>. Either, explicitly set <code>config = list(sparklyr.gateway.address = "&lt;driver-name&gt;")</code> or implicitly <code>sparklyr</code> will read the <code>site-config.xml</code> for the <code>YARN_CONF_DIR</code> environment variable.</p></li>
<li><p>Added <code><a href="../reference/spark_context_config.html">spark_context_config()</a></code> and <code><a href="../reference/hive_context_config.html">hive_context_config()</a></code> to retrieve runtime configurations for the Spark and Hive contexts.</p></li>
<li><p>Added <code>sparklyr.log.console</code> to redirect logs to console, useful to troubleshooting <code>spark_connect</code>.</p></li>
<li><p>Added <code>sparklyr.backend.args</code> as config option to enable passing parameters to the <code>sparklyr</code> backend.</p></li>
<li><p>Improved logging while establishing connections to <code>sparklyr</code>.</p></li>
<li><p>Improved <code><a href="../reference/spark-connections.html">spark_connect()</a></code> performance.</p></li>
<li><p>Implemented new configuration checks to proactively report connection errors in Windows.</p></li>
<li><p>While connecting to spark from Windows, setting the <code>sparklyr.verbose</code> option to <code>TRUE</code> prints detailed configuration steps.</p></li>
<li><p>Added <code>custom_headers</code> to <code><a href="../reference/livy_config.html">livy_config()</a></code> to add custom headers to the REST call to the Livy server</p></li>
</ul>
</div>
<div id="compilation" class="section level3">
<h3 class="hasAnchor">
<a href="#compilation" class="anchor"></a>Compilation</h3>
<ul>
<li><p>Added support for <code>jar_dep</code> in the compilation specification to support additional <code>jars</code> through <code><a href="../reference/spark_compile.html">spark_compile()</a></code>.</p></li>
<li><p><code><a href="../reference/spark_compile.html">spark_compile()</a></code> now prints deprecation warnings.</p></li>
<li><p>Added <code><a href="../reference/download_scalac.html">download_scalac()</a></code> to assist downloading all the Scala compilers required to build using <code>compile_package_jars</code> and provided support for using any <code>scalac</code> minor versions while looking for the right compiler.</p></li>
</ul>
</div>
<div id="backend" class="section level3">
<h3 class="hasAnchor">
<a href="#backend" class="anchor"></a>Backend</h3>
<ul>
<li>Improved backend logging by adding type and session id prefix.</li>
</ul>
</div>
<div id="miscellaneous" class="section level3">
<h3 class="hasAnchor">
<a href="#miscellaneous" class="anchor"></a>Miscellaneous</h3>
<ul>
<li><p><code>copy_to()</code> and <code>sdf_copy_to()</code> auto generate a <code>name</code> when an expression can’t be transformed into a table name.</p></li>
<li><p>Implemented <code>type_sum.jobj()</code> (from tibble) to enable better printing of jobj objects embedded in data frames.</p></li>
<li><p>Added the <code><a href="../reference/spark_home_set.html">spark_home_set()</a></code> function, to help facilitate the setting of the <code>SPARK_HOME</code> environment variable. This should prove useful in teaching environments, when teaching the basics of Spark and sparklyr.</p></li>
<li><p>Added support for the <code>sparklyr.ui.connections</code> option, which adds additional connection options into the new connections dialog. The <code>rstudio.spark.connections</code> option is now deprecated.</p></li>
<li><p>Implemented the “New Connection Dialog” as a Shiny application to be able to support newer versions of RStudio that deprecate current connections UI.</p></li>
</ul>
</div>
<div id="bug-fixes" class="section level3">
<h3 class="hasAnchor">
<a href="#bug-fixes" class="anchor"></a>Bug Fixes</h3>
<ul>
<li><p>When using <code><a href="../reference/spark-connections.html">spark_connect()</a></code> in local clusters, it validates that <code>java</code> exists under <code>JAVA_HOME</code> to help troubleshoot systems that have an incorrect <code>JAVA_HOME</code>.</p></li>
<li><p>Improved <code>argument is of length zero</code> error triggered while retrieving data with no columns to display.</p></li>
<li><p>Fixed <code>Path does not exist</code> referencing <code>hdfs</code> exception during <code>copy_to</code> under systems configured with <code>HADOOP_HOME</code>.</p></li>
<li><p>Fixed session crash after “No status is returned” error by terminating invalid connection and added support to print log trace during this error.</p></li>
<li><p><code>compute()</code> now caches data in memory by default. To revert this beavior use <code>sparklyr.dplyr.compute.nocache</code> set to <code>TRUE</code>.</p></li>
<li><p><code><a href="../reference/spark-connections.html">spark_connect()</a></code> with <code>master = "local"</code> and a given <code>version</code> overrides <code>SPARK_HOME</code> to avoid existing installation mismatches.</p></li>
<li><p>Fixed <code><a href="../reference/spark-connections.html">spark_connect()</a></code> under Windows issue when <code>newInstance0</code> is present in the logs.</p></li>
<li><p>Fixed collecting <code>long</code> type columns when NAs are present (#463).</p></li>
<li><p>Fixed backend issue that affects systems where <code>localhost</code> does not resolve properly to the loopback address.</p></li>
<li><p>Fixed issue collecting data frames containing newlines <code>\n</code>.</p></li>
<li><p>Spark Null objects (objects of class NullType) discovered within numeric vectors are now collected as NAs, rather than lists of NAs.</p></li>
<li><p>Fixed warning while connecting with livy and improved 401 message.</p></li>
<li><p>Fixed issue in <code><a href="../reference/spark_read_parquet.html">spark_read_parquet()</a></code> and other read methods in which <code>spark_normalize_path()</code> would not work in some platforms while loading data using custom protocols like <code>s3n://</code> for Amazon S3.</p></li>
<li><p>Resolved issue in <code>spark_save()</code> / <code>load_table()</code> to support saving / loading data and added path parameter in <code><a href="../reference/spark_load_table.html">spark_load_table()</a></code> for consistency with other functions.</p></li>
</ul>
</div>
</div>
    <div id="sparklyr-0-5-5" class="section level1">
<h1 class="hasAnchor">
<a href="#sparklyr-0-5-5" class="anchor"></a>Sparklyr 0.5.5</h1>
<ul>
<li>Implemented support for <code>connectionViewer</code> interface required in RStudio 1.1 and <code>spark_connect</code> with <code>mode="databricks"</code>.</li>
</ul>
</div>
    <div id="sparklyr-0-5-4" class="section level1">
<h1 class="hasAnchor">
<a href="#sparklyr-0-5-4" class="anchor"></a>Sparklyr 0.5.4</h1>
<ul>
<li>Implemented support for <code>dplyr 0.6</code> and Spark 2.1.x.</li>
</ul>
</div>
    <div id="sparklyr-0-5-3" class="section level1">
<h1 class="hasAnchor">
<a href="#sparklyr-0-5-3" class="anchor"></a>Sparklyr 0.5.3</h1>
<ul>
<li>Implemented support for <code>DBI 0.6</code>.</li>
</ul>
</div>
    <div id="sparklyr-0-5-2" class="section level1">
<h1 class="hasAnchor">
<a href="#sparklyr-0-5-2" class="anchor"></a>Sparklyr 0.5.2</h1>
<ul>
<li><p>Fix to <code>spark_connect</code> affecting Windows users and Spark 1.6.x.</p></li>
<li><p>Fix to Livy connections which would cause connections to fail while connection is on ‘waiting’ state.</p></li>
</ul>
</div>
    <div id="sparklyr-0-5-0" class="section level1">
<h1 class="hasAnchor">
<a href="#sparklyr-0-5-0" class="anchor"></a>Sparklyr 0.5.0</h1>
<ul>
<li><p>Implemented basic authorization for Livy connections using <code>livy_config_auth()</code>.</p></li>
<li><p>Added support to specify additional <code>spark-submit</code> parameters using the <code>sparklyr.shell.args</code> environment variable.</p></li>
<li><p>Renamed <code>sdf_load()</code> and <code>sdf_save()</code> to <code>spark_read()</code> and <code>spark_write()</code> for consistency.</p></li>
<li><p>The functions <code><a href="../reference/tbl_cache.html">tbl_cache()</a></code> and <code><a href="../reference/tbl_uncache.html">tbl_uncache()</a></code> can now be using without requiring the <code>dplyr</code> namespace to be loaded.</p></li>
<li><p><code>spark_read_csv(..., columns = &lt;...&gt;, header = FALSE)</code> should now work as expected – previously, <code>sparklyr</code> would still attempt to normalize the column names provided.</p></li>
<li><p>Support to configure Livy using the <code>livy.</code> prefix in the <code>config.yml</code> file.</p></li>
<li><p>Implemented experimental support for Livy through: <code><a href="../reference/livy_install.html">livy_install()</a></code>, <code><a href="../reference/livy_service.html">livy_service_start()</a></code>, <code><a href="../reference/livy_service.html">livy_service_stop()</a></code> and <code><a href="../reference/spark-connections.html">spark_connect(method = "livy")</a></code>.</p></li>
<li><p>The <code>ml</code> routines now accept <code>data</code> as an optional argument, to support calls of the form e.g. <code><a href="../reference/ml_linear_regression.html">ml_linear_regression(y ~ x, data = data)</a></code>. This should be especially helpful in conjunction with <code><a href="http://dplyr.tidyverse.org/reference/do.html">dplyr::do()</a></code>.</p></li>
<li><p>Spark <code>DenseVector</code> and <code>SparseVector</code> objects are now deserialized as R numeric vectors, rather than Spark objects. This should make it easier to work with the output produced by <code><a href="../reference/sdf_predict.html">sdf_predict()</a></code> with Random Forest models, for example.</p></li>
<li><p>Implemented <code>dim.tbl_spark()</code>. This should ensure that <code>dim()</code>, <code>nrow()</code> and <code>ncol()</code> all produce the expected result with <code>tbl_spark</code>s.</p></li>
<li><p>Improved Spark 2.0 installation in Windows by creating <code>spark-defaults.conf</code> and configuring <code>spark.sql.warehouse.dir</code>.</p></li>
<li><p>Embedded Apache Spark package dependencies to avoid requiring internet connectivity while connecting for the first through <code>spark_connect</code>. The <code>sparklyr.csv.embedded</code> config setting was added to configure a regular expression to match Spark versions where the embedded package is deployed.</p></li>
<li><p>Increased exception callstack and message length to include full error details when an exception is thrown in Spark.</p></li>
<li><p>Improved validation of supported Java versions.</p></li>
<li><p>The <code><a href="../reference/spark_read_csv.html">spark_read_csv()</a></code> function now accepts the <code>infer_schema</code> parameter, controlling whether the columns schema should be inferred from the underlying file itself. Disabling this should improve performance when the schema is known beforehand.</p></li>
<li><p>Added a <code>do_.tbl_spark</code> implementation, allowing for the execution of <code><a href="http://dplyr.tidyverse.org/reference/do.html">dplyr::do</a></code> statements on Spark DataFrames. Currently, the computation is performed in serial across the different groups specified on the Spark DataFrame; in the future we hope to explore a parallel implementation. Note that <code>do_</code> always returns a <code>tbl_df</code> rather than a <code>tbl_spark</code>, as the objects produced within a <code>do_</code> query may not necessarily be Spark objects.</p></li>
<li><p>Improved errors, warnings and fallbacks for unsupported Spark versions.</p></li>
<li><p><code>sparklyr</code> now defaults to <code>tar = "internal"</code> in its calls to <code>untar()</code>. This should help resolve issues some Windows users have seen related to an inability to connect to Spark, which ultimately were caused by a lack of permissions on the Spark installation.</p></li>
<li><p>Resolved an issue where <code>copy_to()</code> and other R =&gt; Spark data transfer functions could fail when the last column contained missing / empty values. (#265)</p></li>
<li><p>Added <code><a href="../reference/sdf_persist.html">sdf_persist()</a></code> as a wrapper to the Spark DataFrame <code>persist()</code> API.</p></li>
<li><p>Resolved an issue where <code>predict()</code> could produce results in the wrong order for large Spark DataFrames.</p></li>
<li><p>Implemented support for <code>na.action</code> with the various Spark ML routines. The value of <code>getOption("na.action")</code> is used by default. Users can customize the <code>na.action</code> argument through the <code>ml.options</code> object accepted by all ML routines.</p></li>
<li><p>On Windows, long paths, and paths containing spaces, are now supported within calls to <code><a href="../reference/spark-connections.html">spark_connect()</a></code>.</p></li>
<li><p>The <code>lag()</code> window function now accepts numeric values for <code>n</code>. Previously, only integer values were accepted. (#249)</p></li>
<li><p>Added support to configure Ppark environment variables using <code>spark.env.*</code> config.</p></li>
<li><p>Added support for the <code>Tokenizer</code> and <code>RegexTokenizer</code> feature transformers. These are exported as the <code><a href="../reference/ft_tokenizer.html">ft_tokenizer()</a></code> and <code><a href="../reference/ft_regex_tokenizer.html">ft_regex_tokenizer()</a></code> functions.</p></li>
<li><p>Resolved an issue where attempting to call <code>copy_to()</code> with an R <code>data.frame</code> containing many columns could fail with a Java StackOverflow. (#244)</p></li>
<li><p>Resolved an issue where attempting to call <code>collect()</code> on a Spark DataFrame containing many columns could produce the wrong result. (#242)</p></li>
<li><p>Added support to parameterize network timeouts using the <code>sparklyr.backend.timeout</code>, <code>sparklyr.gateway.start.timeout</code> and <code>sparklyr.gateway.connect.timeout</code> config settings.</p></li>
<li><p>Improved logging while establishing connections to <code>sparklyr</code>.</p></li>
<li><p>Added <code>sparklyr.gateway.port</code> and <code>sparklyr.gateway.address</code> as config settings.</p></li>
<li><p>The <code><a href="../reference/spark_log.html">spark_log()</a></code> function now accepts the <code>filter</code> parameter. This can be used to filter entries within the Spark log.</p></li>
<li><p>Increased network timeout for <code>sparklyr.backend.timeout</code>.</p></li>
<li><p>Moved <code>spark.jars.default</code> setting from options to Spark config.</p></li>
<li><p><code>sparklyr</code> now properly respects the Hive metastore directory with the <code><a href="../reference/sdf-saveload.html">sdf_save_table()</a></code> and <code><a href="../reference/sdf-saveload.html">sdf_load_table()</a></code> APIs for Spark &lt; 2.0.0.</p></li>
<li><p>Added <code><a href="../reference/sdf_quantile.html">sdf_quantile()</a></code> as a means of computing (approximate) quantiles for a column of a Spark DataFrame.</p></li>
<li><p>Added support for <code>n_distinct(...)</code> within the <code>dplyr</code> interface, based on call to Hive function <code>count(DISTINCT ...)</code>. (#220)</p></li>
</ul>
</div>
    <div id="sparklyr-0-4-0" class="section level1">
<h1 class="hasAnchor">
<a href="#sparklyr-0-4-0" class="anchor"></a>Sparklyr 0.4.0</h1>
<ul>
<li>First release to CRAN.</li>
</ul>
</div>
    </div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
    <div id="tocnav">
      <h2>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
        <li><a href="#sparklyr-0-7-unreleased">0.7</a></li>
        <li><a href="#sparklyr-0-6-1-unreleased">0.6.1</a></li>
        <li><a href="#sparklyr-0-6-0">0.6.0</a></li>
        <li><a href="#sparklyr-0-5-5">0.5.5</a></li>
        <li><a href="#sparklyr-0-5-4">0.5.4</a></li>
        <li><a href="#sparklyr-0-5-3">0.5.3</a></li>
        <li><a href="#sparklyr-0-5-2">0.5.2</a></li>
        <li><a href="#sparklyr-0-5-0">0.5.0</a></li>
        <li><a href="#sparklyr-0-4-0">0.4.0</a></li>
      </ul>
    </div>
  </div>

</div>

      <footer>
      <div class="copyright">
  <p>Developed by Javier Luraschi, Kevin Ushey, JJ Allaire,  The Apache Software Foundation.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
   </div>

  </body>
</html>
