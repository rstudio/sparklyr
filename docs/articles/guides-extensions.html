<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Creating Extensions for sparklyr • sparklyr</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">sparklyr</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">Home</a>
</li>
<li>
  <a href="../articles/gallery.html">Gallery</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Guides
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/guides-dplyr.html">dplyr</a>
    </li>
    <li>
      <a href="../articles/guides-mllib.html">mllib</a>
    </li>
    <li>
      <a href="../articles/guides-distributed-r.html">Distributed R</a>
    </li>
    <li>
      <a href="../articles/guides-extensions.html">Extensions</a>
    </li>
    <li>
      <a href="../articles/guides-caching.html">Caching</a>
    </li>
    <li>
      <a href="../articles/guides-h2o.html">H2O</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Deployment
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/deployment-overview.html">Overview</a>
    </li>
    <li>
      <a href="../articles/deployment-data-lakes.html">Data Lakes</a>
    </li>
    <li>
      <a href="../articles/deployment-cdh.html">Cloudera</a>
    </li>
    <li>
      <a href="../articles/deployment-amazon.html">Amazon</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Reference
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../reference/index.html">Functions</a>
    </li>
    <li>
      <a href="https://github.com/rstudio/cheatsheets/raw/master/source/pdfs/sparklyr-cheatsheet.pdf">Cheatsheet</a>
    </li>
    <li>
      <a href="../articles/reference-media.html">Media</a>
    </li>
    <li>
      <a href="../news/index.html">News</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/rstudio/sparklyr">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Creating Extensions for sparklyr</h1>
            
          </div>

    
    
<div class="contents">
<div id="introduction" class="section level2">
<h2 class="hasAnchor">
<a href="#introduction" class="anchor"></a>Introduction</h2>
<p>The sparklyr package provides a <a href="dplyr.html">dplyr</a> interface to Spark DataFrames as well as an R interface to Spark’s distributed <a href="mllib.html">machine learning</a> pipelines. However, since Spark is a general-purpose cluster computing system there are many other R interfaces that could be built (e.g. interfaces to custom machine learning pipelines, interfaces to 3rd party Spark packages, etc.).</p>
<p>The facilities used internally by sparklyr for its dplyr and machine learning interfaces are available to extension packages. This guide describes how you can use these tools to create your own custom R interfaces to Spark.</p>
<div id="examples" class="section level3">
<h3 class="hasAnchor">
<a href="#examples" class="anchor"></a>Examples</h3>
<p>Here’s an example of an extension function that calls the text file line counting function available via the SparkContext:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(sparklyr)
count_lines &lt;-<span class="st"> </span><span class="cf">function</span>(sc, file) {
  <span class="kw"><a href="../reference/spark-api.html">spark_context</a></span>(sc) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">invoke</span>(<span class="st">"textFile"</span>, file, 1L) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">invoke</span>(<span class="st">"count"</span>)
}</code></pre></div>
<p>The <code>count_lines</code> function takes a <code>spark_connection</code> (<code>sc</code>) argument which enables it to obtain a reference to the <code>SparkContext</code> object, and in turn call the <code>textFile().count()</code> method.</p>
<p>You can use this function with an existing sparklyr connection as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(sparklyr)
sc &lt;-<span class="st"> </span><span class="kw"><a href="../reference/spark-connections.html">spark_connect</a></span>(<span class="dt">master =</span> <span class="st">"local"</span>)
<span class="kw">count_lines</span>(sc, <span class="st">"hdfs://path/data.csv"</span>)</code></pre></div>
<p>Here are links to some additional examples of extension packages:</p>
<table class="table">
<colgroup>
<col width="38%">
<col width="61%">
</colgroup>
<thead><tr class="header">
<th>Package</th>
<th>Description</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="https://github.com/bnosac/spark.sas7bdat"><code>spark.sas7bdat</code></a></td>
<td>Read in SAS data in parallel into Apache Spark.</td>
</tr>
<tr class="even">
<td><a href="https://github.com/h2oai/rsparkling"><code>rsparkling</code></a></td>
<td>Extension for using <a href="h2o.ai">H2O</a> machine learning algorithms against Spark Data Frames.</td>
</tr>
<tr class="odd">
<td><a href="https://github.com/javierluraschi/sparkhello"><code>sparkhello</code></a></td>
<td>Simple example of including a custom JAR file within an extension package.</td>
</tr>
<tr class="even">
<td><a href="https://github.com/clarkfitzg/rddlist"><code>rddlist</code></a></td>
<td>Implements some methods of an R list as a Spark RDD (resilient distributed dataset).</td>
</tr>
<tr class="odd">
<td><a href="https://github.com/javierluraschi/sparkwarc"><code>sparkwarc</code></a></td>
<td>Load <a href="http://commoncrawl.org/the-data/get-started/">WARC files</a> into Apache Spark with sparklyr.</td>
</tr>
<tr class="even">
<td><a href="https://github.com/chezou/sparkavro"><code>sparkavro</code></a></td>
<td>Load Avro data into Spark with sparklyr. It is a wrapper of <a href="https://github.com/databricks/spark-avro">spark-avro</a>
</td>
</tr>
<tr class="odd">
<td><a href="https://github.com/AkhilNairAmey/crassy"><code>crassy</code></a></td>
<td>Connect to Cassandra with sparklyr using the <code>Spark-Cassandra-Connector</code>.</td>
</tr>
<tr class="even">
<td><a href="https://github.com/kevinykuo/sparklygraphs"><code>sparklygraphs</code></a></td>
<td>R interface for <a href="https://graphframes.github.io/">GraphFrames</a> which aims to provide the functionality of <a href="http://spark.apache.org/graphx/">GraphX</a>.</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="core-types" class="section level2">
<h2 class="hasAnchor">
<a href="#core-types" class="anchor"></a>Core Types</h2>
<p>Three classes are defined for representing the fundamental types of the R to Java bridge:</p>
<table class="table">
<thead><tr class="header">
<th>Function</th>
<th>Description</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="reference/sparklyr/latest/spark_connection.html"><code>spark_connection</code></a></td>
<td>Connection between R and the Spark shell process</td>
</tr>
<tr class="even">
<td><a href="reference/sparklyr/latest/spark_jobj.html"><code>spark_jobj</code></a></td>
<td>Instance of a remote Spark object</td>
</tr>
<tr class="odd">
<td><a href="reference/sparklyr/latest/spark_dataframe.html"><code>spark_dataframe</code></a></td>
<td>Instance of a remote Spark DataFrame object</td>
</tr>
</tbody>
</table>
<p>S3 methods are defined for each of these classes so they can be easily converted to or from objects that contain or wrap them. Note that for any given <code>spark_jobj</code> it’s possible to discover the underlying <code>spark_connection</code>.</p>
</div>
<div id="calling-spark-from-r" class="section level2">
<h2 class="hasAnchor">
<a href="#calling-spark-from-r" class="anchor"></a>Calling Spark from R</h2>
<p>There are several functions available for calling the methods of Java objects and static methods of Java classes:</p>
<table class="table">
<thead><tr class="header">
<th>Function</th>
<th>Description</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="reference/sparklyr/latest/invoke.html"><code>invoke</code></a></td>
<td>Call a method on an object</td>
</tr>
<tr class="even">
<td><a href="reference/sparklyr/latest/invoke.html"><code>invoke_new</code></a></td>
<td>Create a new object by invoking a constructor</td>
</tr>
<tr class="odd">
<td><a href="reference/sparklyr/latest/invoke.html"><code>invoke_static</code></a></td>
<td>Call a static method on an object</td>
</tr>
</tbody>
</table>
<p>For example, to create a new instance of the <code>java.math.BigInteger</code> class and then call the <code>longValue()</code> method on it you would use code like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">billionBigInteger &lt;-<span class="st"> </span><span class="kw"><a href="../reference/invoke.html">invoke_new</a></span>(sc, <span class="st">"java.math.BigInteger"</span>, <span class="st">"1000000000"</span>)
billion &lt;-<span class="st"> </span><span class="kw">invoke</span>(billionBigInteger, <span class="st">"longValue"</span>)</code></pre></div>
<p>Note the <code>sc</code> argument: that’s the <code>spark_connection</code> object which is provided by the front-end package (e.g. sparklyr).</p>
<p>The previous example can be re-written to be more compact and clear using <a href="https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html">magrittr</a> pipes:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">billion &lt;-<span class="st"> </span>sc <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw"><a href="../reference/invoke.html">invoke_new</a></span>(<span class="st">"java.math.BigInteger"</span>, <span class="st">"1000000000"</span>) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">invoke</span>(<span class="st">"longValue"</span>)</code></pre></div>
<p>This syntax is similar to the method-chaining syntax often used with Scala code so is generally preferred.</p>
<p>Calling a static method of a class is also straightforward. For example, to call the <code>Math::hypot()</code> static function you would use this code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hypot &lt;-<span class="st"> </span>sc <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw"><a href="../reference/invoke.html">invoke_static</a></span>(<span class="st">"java.lang.Math"</span>, <span class="st">"hypot"</span>, <span class="dv">10</span>, <span class="dv">20</span>) </code></pre></div>
</div>
<div id="wrapper-functions" class="section level2">
<h2 class="hasAnchor">
<a href="#wrapper-functions" class="anchor"></a>Wrapper Functions</h2>
<p>Creating an extension typically consists of writing R wrapper functions for a set of Spark services. In this section we’ll describe the typical form of these functions as well as how to handle special types like Spark DataFrames.</p>
<p>Here’s the wrapper function for <code>textFile().count()</code> which we defined earlier:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">count_lines &lt;-<span class="st"> </span><span class="cf">function</span>(sc, file) {
  <span class="kw"><a href="../reference/spark-api.html">spark_context</a></span>(sc) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">invoke</span>(<span class="st">"textFile"</span>, file, 1L) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">      </span><span class="kw">invoke</span>(<span class="st">"count"</span>)
}</code></pre></div>
<p>The <code>count_lines</code> function takes a <code>spark_connection</code> (<code>sc</code>) argument which enables it to obtain a reference to the <code>SparkContext</code> object, and in turn call the <code>textFile().count()</code> method.</p>
<p>The following functions are useful for implementing wrapper functions of various kinds:</p>
<table class="table">
<colgroup>
<col width="38%">
<col width="61%">
</colgroup>
<thead><tr class="header">
<th>Function</th>
<th>Description</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="reference/sparklyr/latest/spark_connection.html"><code>spark_connection</code></a></td>
<td>Get the Spark connection associated with an object (S3)</td>
</tr>
<tr class="even">
<td><a href="reference/sparklyr/latest/spark_jobj.html"><code>spark_jobj</code></a></td>
<td>Get the Spark jobj associated with an object (S3)</td>
</tr>
<tr class="odd">
<td><a href="reference/sparklyr/latest/spark_dataframe.html"><code>spark_dataframe</code></a></td>
<td>Get the Spark DataFrame associated with an object (S3)</td>
</tr>
<tr class="even">
<td><a href="reference/sparklyr/latest/spark_context.html"><code>spark_context</code></a></td>
<td>Get the SparkContext for a <code>spark_connection</code>
</td>
</tr>
<tr class="odd">
<td><a href="reference/sparklyr/latest/hive_context.html"><code>hive_context</code></a></td>
<td>Get the HiveContext for a <code>spark_connection</code>
</td>
</tr>
<tr class="even">
<td><a href="reference/sparklyr/latest/spark_version.html"><code>spark_version</code></a></td>
<td>Get the version of Spark (as a <code>numeric_version</code>) for a <code>spark_connection</code>
</td>
</tr>
</tbody>
</table>
<p>The use of these functions is illustrated in this simple example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">analyze &lt;-<span class="st"> </span><span class="cf">function</span>(x, features) {
  
  <span class="co"># normalize whatever we were passed (e.g. a dplyr tbl) into a DataFrame</span>
  df &lt;-<span class="st"> </span><span class="kw"><a href="../reference/spark_dataframe.html">spark_dataframe</a></span>(x)
  
  <span class="co"># get the underlying connection so we can create new objects</span>
  sc &lt;-<span class="st"> </span><span class="kw"><a href="../reference/spark_connection.html">spark_connection</a></span>(df)
  
  <span class="co"># create an object to do the analysis and call its `analyze` and `summary`</span>
  <span class="co"># methods (note that the df and features are passed to the analyze function)</span>
  summary &lt;-<span class="st"> </span>sc <span class="op">%&gt;%</span><span class="st">  </span>
<span class="st">    </span><span class="kw"><a href="../reference/invoke.html">invoke_new</a></span>(<span class="st">"com.example.tools.Analyzer"</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">      </span><span class="kw">invoke</span>(<span class="st">"analyze"</span>, df, features) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">      </span><span class="kw">invoke</span>(<span class="st">"summary"</span>)

  <span class="co"># return the results</span>
  summary
}</code></pre></div>
<p>The first argument is an object that can be accessed using the Spark DataFrame API (this might be an actual reference to a DataFrame or could rather be a dplyr <code>tbl</code> which has a DataFrame reference inside it).</p>
<p>After using the <code>spark_dataframe</code> function to normalize the reference, we extract the underlying Spark connection associated with the data frame using the <code>spark_connection</code> function. Finally, we create a new <code>Analyzer</code> object, call it’s <code>analyze</code> method with the DataFrame and list of features, and then call the <code>summary</code> method on the results of the analysis.</p>
<p>Accepting a <code>spark_jobj</code> or <code>spark_dataframe</code> as the first argument of a function makes it very easy to incorporate into magrittr pipelines so this pattern is highly recommended when possible.</p>
</div>
<div id="dependencies" class="section level2">
<h2 class="hasAnchor">
<a href="#dependencies" class="anchor"></a>Dependencies</h2>
<p>When creating R packages which implement interfaces to Spark you may need to include additional dependencies. Your dependencies might be a set of <a href="https://spark-packages.org/">Spark Packages</a> or might be a custom JAR file. In either case, you’ll need a way to specify that these dependencies should be included during the initialization of a Spark session. A Spark dependency is defined using the <code>spark_dependency</code> function:</p>
<table class="table">
<colgroup>
<col width="38%">
<col width="61%">
</colgroup>
<thead><tr class="header">
<th>Function</th>
<th>Description</th>
</tr></thead>
<tbody><tr class="odd">
<td><a href="reference/sparklyr/latest/spark_dependency"><code>spark_dependency</code></a></td>
<td>Define a Spark dependency consisting of JAR files and Spark packages</td>
</tr></tbody>
</table>
<p>Your extension package can specify it’s dependencies by implementing a function named <code>spark_dependencies</code> within the package (this function should <em>not</em> be publicly exported). For example, let’s say you were creating an extension package named <strong>sparkds</strong> that needs to include a custom JAR as well as the Redshift and Apache Avro packages:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">spark_dependencies &lt;-<span class="st"> </span><span class="cf">function</span>(spark_version, scala_version, ...) {
  <span class="kw"><a href="../reference/spark_dependency.html">spark_dependency</a></span>(
    <span class="dt">jars =</span> <span class="kw">c</span>(
      <span class="kw">system.file</span>(
        <span class="kw">sprintf</span>(<span class="st">"java/sparkds-%s-%s.jar"</span>, spark_version, scala_version), 
        <span class="dt">package =</span> <span class="st">"sparkds"</span>
      )
    ),
    <span class="dt">packages =</span> <span class="kw">c</span>(
      <span class="kw">sprintf</span>(<span class="st">"com.databricks:spark-redshift_%s:0.6.0"</span>, scala_version),
      <span class="kw">sprintf</span>(<span class="st">"com.databricks:spark-avro_%s:2.0.1"</span>, scala_version)
    )
  )
}

.onLoad &lt;-<span class="st"> </span><span class="cf">function</span>(libname, pkgname) {
  sparklyr<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/sparklyr/topics/register_extension">register_extension</a></span>(pkgname)
}</code></pre></div>
<p>The <code>spark_version</code> argument is provided so that a package can support multiple Spark versions for it’s JARs. Note that the argument will include just the major and minor versions (e.g. <code>1.6</code> or <code>2.0</code>) and will not include the patch level (as JARs built for a given major/minor version are expected to work for all patch levels).</p>
<p>The <code>scala_version</code> argument is provided so that a single package can support multiple Scala compiler versions for it’s JARs and packages (currently Scala 1.6 downloadable binaries are compiled with Scala 2.10 and Scala 2.0 downloadable binaries are compiled with Scala 2.11).</p>
<p>The <code>...</code> argument is unused but nevertheless should be included to ensure compatibility if new arguments are added to <code>spark_dependencies</code> in the future.</p>
<p>The <code>.onLoad</code> function registers your extension package so that it’s <code>spark_dependencies</code> function will be automatically called when new connections to Spark are made via <code>spark_connect</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(sparklyr)
<span class="kw">library</span>(sparkds)
sc &lt;-<span class="st"> </span><span class="kw"><a href="../reference/spark-connections.html">spark_connect</a></span>(<span class="dt">master =</span> <span class="st">"local"</span>)</code></pre></div>
<div id="compiling-jars" class="section level3">
<h3 class="hasAnchor">
<a href="#compiling-jars" class="anchor"></a>Compiling JARs</h3>
<p>The <strong>sparklyr</strong> package includes a utility function (<code>compile_package_jars</code>) that will automatically compile a JAR file from your Scala source code for the required permutations of Spark and Scala compiler versions. To use the function just invoke it from the root directory of your R package as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sparklyr<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/sparklyr/topics/compile_package_jars">compile_package_jars</a></span>()</code></pre></div>
<p>Note that a prerequisite to calling <code>compile_package_jars</code> is the installation of the Scala 2.10 and 2.11 compilers to one of the following paths:</p>
<ul>
<li>/opt/scala</li>
<li>/opt/local/scala</li>
<li>/usr/local/scala</li>
<li>~/scala (Windows-only)</li>
</ul>
<p>See the <a href="https://github.com/jjallaire/sparkhello">sparkhello</a> repository for a complete example of including a custom JAR within an extension package.</p>
<div id="cran" class="section level4">
<h4 class="hasAnchor">
<a href="#cran" class="anchor"></a>CRAN</h4>
<p>When including a JAR file within an R package distributed on CRAN, you should follow the guidelines provided in <a href="https://cran.r-project.org/doc/manuals/r-release/R-exts.html#Non_002dR-scripts-in-packages">Writing R Extensions</a>:</p>
<blockquote>
<p>Java code is a special case: except for very small programs, .java files should be byte-compiled (to a .class file) and distributed as part of a .jar file: the conventional location for the .jar file(s) is <code>inst/java</code>. It is desirable (and required under an Open Source license) to make the Java source files available: this is best done in a top-level <code>java</code> directory in the package – the source files should not be installed.</p>
</blockquote>
</div>
</div>
</div>
<div id="data-types" class="section level2">
<h2 class="hasAnchor">
<a href="#data-types" class="anchor"></a>Data Types</h2>
<p>The <a href="http://spark.rstudio.com/reference/sparklyr/latest/ensure.html"><code>ensure_*</code></a> family of functions can be used to enforce specific data types that are passed to a Spark routine. For example, Spark routines that require an integer will not accept an R numeric element. Use these functions ensure certain parameters are scalar integers, or scalar doubles, and so on.</p>
<ul>
<li>ensure_scalar_integer</li>
<li>ensure_scalar_double</li>
<li>ensure_scalar_boolean</li>
<li>ensure_scalar_character</li>
</ul>
</div>
<div id="compiling" class="section level2">
<h2 class="hasAnchor">
<a href="#compiling" class="anchor"></a>Compiling</h2>
<p>Most Spark extensions won’t need to define their own compilation specification, and can instead rely on the default behavior of <code>compile_package_jars</code>. For users who would like to take more control over where the scalac compilers should be looked up, use the <a href="http://spark.rstudio.com/reference/sparklyr/latest/spark_compilation_spec.html"><code>spark_compilation_spec</code></a> fucnction. The Spark compilation specification is used when compiling Spark extension Java Archives, and defines which versions of Spark, as well as which versions of Scala, should be used for compilation.</p>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#introduction">Introduction</a></li>
      <li><a href="#core-types">Core Types</a></li>
      <li><a href="#calling-spark-from-r">Calling Spark from R</a></li>
      <li><a href="#wrapper-functions">Wrapper Functions</a></li>
      <li><a href="#dependencies">Dependencies</a></li>
      <li><a href="#data-types">Data Types</a></li>
      <li><a href="#compiling">Compiling</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Javier Luraschi, Kevin Ushey, JJ Allaire,  The Apache Software Foundation.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
