<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>R Interface to Apache Spark • sparklyr</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="pkgdown.css" rel="stylesheet">
<script src="jquery.sticky-kit.min.js"></script><script src="pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-home">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">sparklyr</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="articles/gallery.html">Gallery</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Guides
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="articles/guides-dplyr.html">dplyr</a>
    </li>
    <li>
      <a href="articles/guides-mllib.html">mllib</a>
    </li>
    <li>
      <a href="articles/guides-distributed-r.html">Distributed R</a>
    </li>
    <li>
      <a href="articles/guides-extensions.html">Extensions</a>
    </li>
    <li>
      <a href="articles/guides-caching.html">Caching</a>
    </li>
    <li>
      <a href="articles/guides-h2o.html">H2O</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Deployment
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="articles/deployment-overview.html">Overview</a>
    </li>
    <li>
      <a href="articles/deployment-data-lakes.html">Data Lakes</a>
    </li>
    <li>
      <a href="articles/deployment-cdh.html">Cloudera</a>
    </li>
    <li>
      <a href="articles/deployment-amazon.html">Amazon</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Reference
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="reference/index.html">Functions</a>
    </li>
    <li>
      <a href="https://github.com/rstudio/cheatsheets/raw/master/source/pdfs/sparklyr-cheatsheet.pdf">Cheatsheet</a>
    </li>
    <li>
      <a href="articles/reference-media.html">Media</a>
    </li>
    <li>
      <a href="news/index.html">News</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/rstudio/sparklyr">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="contents col-md-9">
    <div id="sparklyr-r-interface-for-apache-spark" class="section level1">
<div class="page-header"><h1 class="hasAnchor">
<a href="#sparklyr-r-interface-for-apache-spark" class="anchor"></a>sparklyr: R interface for Apache Spark</h1></div>

<p><img src="tools/readme/sparklyr-illustration.png" width="364" height="197" align="right"></p>
<ul>
<li>Connect to <a href="http://spark.apache.org/">Spark</a> from R. The sparklyr package provides a <br> complete <a href="https://github.com/hadley/dplyr">dplyr</a> backend.</li>
<li>Filter and aggregate Spark datasets then bring them into R for <br> analysis and visualization.</li>
<li>Use Spark’s distributed <a href="http://spark.apache.org/docs/latest/mllib-guide.html">machine learning</a> library from R.</li>
<li>Create <a href="http://spark.rstudio.com/extensions.html">extensions</a> that call the full Spark API and provide <br> interfaces to Spark packages.</li>
</ul>
<div id="installation" class="section level2">
<h2 class="hasAnchor">
<a href="#installation" class="anchor"></a>Installation</h2>
<p>You can install the <strong>sparklyr</strong> package from CRAN as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">install.packages</span>(<span class="st">"sparklyr"</span>)</code></pre></div>
<p>You should also install a local version of Spark for development purposes:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(sparklyr)
<span class="kw"><a href="reference/spark_install.html">spark_install</a></span>(<span class="dt">version =</span> <span class="st">"2.1.0"</span>)</code></pre></div>
<p>To upgrade to the latest version of sparklyr, run the following command and restart your r session:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/devtools/topics/install_github">install_github</a></span>(<span class="st">"rstudio/sparklyr"</span>)</code></pre></div>
<p>If you use the RStudio IDE, you should also download the latest <a href="https://www.rstudio.com/products/rstudio/download/preview/">preview release</a> of the IDE which includes several enhancements for interacting with Spark (see the <a href="#rstudio-ide">RStudio IDE</a> section below for more details).</p>
</div>
<div id="connecting-to-spark" class="section level2">
<h2 class="hasAnchor">
<a href="#connecting-to-spark" class="anchor"></a>Connecting to Spark</h2>
<p>You can connect to both local instances of Spark as well as remote Spark clusters. Here we’ll connect to a local instance of Spark via the <a href="http://spark.rstudio.com/reference/sparklyr/latest/spark_connect.html">spark_connect</a> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(sparklyr)
sc &lt;-<span class="st"> </span><span class="kw"><a href="reference/spark-connections.html">spark_connect</a></span>(<span class="dt">master =</span> <span class="st">"local"</span>)</code></pre></div>
<p>The returned Spark connection (<code>sc</code>) provides a remote dplyr data source to the Spark cluster.</p>
<p>For more information on connecting to remote Spark clusters see the <a href="http://spark.rstudio.com/deployment.html">Deployment</a> section of the sparklyr website.</p>
</div>
<div id="using-dplyr" class="section level2">
<h2 class="hasAnchor">
<a href="#using-dplyr" class="anchor"></a>Using dplyr</h2>
<p>We can now use all of the available dplyr verbs against the tables within the cluster.</p>
<p>We’ll start by copying some datasets from R into the Spark cluster (note that you may need to install the nycflights13 and Lahman packages in order to execute this code):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">install.packages</span>(<span class="kw">c</span>(<span class="st">"nycflights13"</span>, <span class="st">"Lahman"</span>))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(dplyr)
iris_tbl &lt;-<span class="st"> </span><span class="kw">copy_to</span>(sc, iris)
flights_tbl &lt;-<span class="st"> </span><span class="kw">copy_to</span>(sc, nycflights13<span class="op">::</span>flights, <span class="st">"flights"</span>)
batting_tbl &lt;-<span class="st"> </span><span class="kw">copy_to</span>(sc, Lahman<span class="op">::</span>Batting, <span class="st">"batting"</span>)
<span class="kw">src_tbls</span>(sc)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode R"><code class="sourceCode r">## [1] "batting" "flights" "iris"</code></pre></div>
<p>To start with here’s a simple filtering example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># filter by departure delay and print the first few records</span>
flights_tbl <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(dep_delay <span class="op">==</span><span class="st"> </span><span class="dv">2</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode R"><code class="sourceCode r">## # Source:   lazy query [?? x 19]
## # Database: spark_connection
##     year month   day dep_time sched_dep_time dep_delay arr_time
##    &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;
##  1  2013     1     1      517            515         2      830
##  2  2013     1     1      542            540         2      923
##  3  2013     1     1      702            700         2     1058
##  4  2013     1     1      715            713         2      911
##  5  2013     1     1      752            750         2     1025
##  6  2013     1     1      917            915         2     1206
##  7  2013     1     1      932            930         2     1219
##  8  2013     1     1     1028           1026         2     1350
##  9  2013     1     1     1042           1040         2     1325
## 10  2013     1     1     1231           1229         2     1523
## # ... with 6,223 more rows, and 12 more variables: sched_arr_time &lt;int&gt;,
## #   arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;,
## #   origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;,
## #   minute &lt;dbl&gt;, time_hour &lt;dbl&gt;</code></pre></div>
<p><a href="https://CRAN.R-project.org/package=dplyr">Introduction to dplyr</a> provides additional dplyr examples you can try. For example, consider the last example from the tutorial which plots data on flight delays:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">delay &lt;-<span class="st"> </span>flights_tbl <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(tailnum) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">count =</span> <span class="kw">n</span>(), <span class="dt">dist =</span> <span class="kw">mean</span>(distance), <span class="dt">delay =</span> <span class="kw">mean</span>(arr_delay)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(count <span class="op">&gt;</span><span class="st"> </span><span class="dv">20</span>, dist <span class="op">&lt;</span><span class="st"> </span><span class="dv">2000</span>, <span class="op">!</span><span class="kw">is.na</span>(delay)) <span class="op">%&gt;%</span>
<span class="st">  </span>collect

<span class="co"># plot delays</span>
<span class="kw">library</span>(ggplot2)
<span class="kw">ggplot</span>(delay, <span class="kw">aes</span>(dist, delay)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">size =</span> count), <span class="dt">alpha =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_size_area</span>(<span class="dt">max_size =</span> <span class="dv">2</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode R"><code class="sourceCode r">## `geom_smooth()` using method = 'gam'</code></pre></div>
<p><img src="tools/readme/ggplot2-1.png"></p>
<div id="window-functions" class="section level3">
<h3 class="hasAnchor">
<a href="#window-functions" class="anchor"></a>Window Functions</h3>
<p>dplyr <a href="https://CRAN.R-project.org/package=dplyr">window functions</a> are also supported, for example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">batting_tbl <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(playerID, yearID, teamID, G, AB<span class="op">:</span>H) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(playerID, yearID, teamID) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(playerID) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(<span class="kw">min_rank</span>(<span class="kw">desc</span>(H)) <span class="op">&lt;=</span><span class="st"> </span><span class="dv">2</span> <span class="op">&amp;</span><span class="st"> </span>H <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode R"><code class="sourceCode r">## # Source:     lazy query [?? x 7]
## # Database:   spark_connection
## # Groups:     playerID
## # Ordered by: playerID, yearID, teamID
##     playerID yearID teamID     G    AB     R     H
##        &lt;chr&gt;  &lt;int&gt;  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;
##  1 aaronha01   1959    ML1   154   629   116   223
##  2 aaronha01   1963    ML1   161   631   121   201
##  3 abbotji01   1999    MIL    20    21     0     2
##  4 abnersh01   1992    CHA    97   208    21    58
##  5 abnersh01   1990    SDN    91   184    17    45
##  6 acklefr01   1963    CHA     2     5     0     1
##  7 acklefr01   1964    CHA     3     1     0     1
##  8 adamecr01   2015    COL    26    53     4    13
##  9 adamecr01   2014    COL     7    15     1     1
## 10 adamsac01   1943    NY1    70    32     3     4
## # ... with 2.561e+04 more rows</code></pre></div>
<p>For additional documentation on using dplyr with Spark see the <a href="http://spark.rstudio.com/dplyr.html">dplyr</a> section of the sparklyr website.</p>
</div>
</div>
<div id="using-sql" class="section level2">
<h2 class="hasAnchor">
<a href="#using-sql" class="anchor"></a>Using SQL</h2>
<p>It’s also possible to execute SQL queries directly against tables within a Spark cluster. The <code>spark_connection</code> object implements a <a href="https://github.com/rstats-db/DBI">DBI</a> interface for Spark, so you can use <code>dbGetQuery</code> to execute SQL and return the result as an R data frame:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(DBI)
iris_preview &lt;-<span class="st"> </span><span class="kw">dbGetQuery</span>(sc, <span class="st">"SELECT * FROM iris LIMIT 10"</span>)
iris_preview</code></pre></div>
<div class="sourceCode"><pre class="sourceCode R"><code class="sourceCode r">##    Sepal_Length Sepal_Width Petal_Length Petal_Width Species
## 1           5.1         3.5          1.4         0.2  setosa
## 2           4.9         3.0          1.4         0.2  setosa
## 3           4.7         3.2          1.3         0.2  setosa
## 4           4.6         3.1          1.5         0.2  setosa
## 5           5.0         3.6          1.4         0.2  setosa
## 6           5.4         3.9          1.7         0.4  setosa
## 7           4.6         3.4          1.4         0.3  setosa
## 8           5.0         3.4          1.5         0.2  setosa
## 9           4.4         2.9          1.4         0.2  setosa
## 10          4.9         3.1          1.5         0.1  setosa</code></pre></div>
</div>
<div id="machine-learning" class="section level2">
<h2 class="hasAnchor">
<a href="#machine-learning" class="anchor"></a>Machine Learning</h2>
<p>You can orchestrate machine learning algorithms in a Spark cluster via the <a href="http://spark.apache.org/docs/latest/mllib-guide.html">machine learning</a> functions within <strong>sparklyr</strong>. These functions connect to a set of high-level APIs built on top of DataFrames that help you create and tune machine learning workflows.</p>
<p>Here’s an example where we use <a href="http://spark.rstudio.com/reference/sparklyr/latest/ml_linear_regression.html">ml_linear_regression</a> to fit a linear regression model. We’ll use the built-in <code>mtcars</code> dataset, and see if we can predict a car’s fuel consumption (<code>mpg</code>) based on its weight (<code>wt</code>), and the number of cylinders the engine contains (<code>cyl</code>). We’ll assume in each case that the relationship between <code>mpg</code> and each of our features is linear.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># copy mtcars into spark</span>
mtcars_tbl &lt;-<span class="st"> </span><span class="kw">copy_to</span>(sc, mtcars)

<span class="co"># transform our data set, and then partition into 'training', 'test'</span>
partitions &lt;-<span class="st"> </span>mtcars_tbl <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(hp <span class="op">&gt;=</span><span class="st"> </span><span class="dv">100</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">cyl8 =</span> cyl <span class="op">==</span><span class="st"> </span><span class="dv">8</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="reference/sdf_partition.html">sdf_partition</a></span>(<span class="dt">training =</span> <span class="fl">0.5</span>, <span class="dt">test =</span> <span class="fl">0.5</span>, <span class="dt">seed =</span> <span class="dv">1099</span>)

<span class="co"># fit a linear model to the training dataset</span>
fit &lt;-<span class="st"> </span>partitions<span class="op">$</span>training <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="reference/ml_linear_regression.html">ml_linear_regression</a></span>(<span class="dt">response =</span> <span class="st">"mpg"</span>, <span class="dt">features =</span> <span class="kw">c</span>(<span class="st">"wt"</span>, <span class="st">"cyl"</span>))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode R"><code class="sourceCode r">## * No rows dropped by 'na.omit' call</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit</code></pre></div>
<div class="sourceCode"><pre class="sourceCode R"><code class="sourceCode r">## Call: ml_linear_regression(., response = "mpg", features = c("wt", "cyl"))
## 
## Coefficients:
## (Intercept)          wt         cyl 
##   33.499452   -2.818463   -0.923187</code></pre></div>
<p>For linear regression models produced by Spark, we can use <code>summary()</code> to learn a bit more about the quality of our fit, and the statistical significance of each of our predictors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(fit)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode R"><code class="sourceCode r">## Call: ml_linear_regression(., response = "mpg", features = c("wt", "cyl"))
## 
## Deviance Residuals::
##    Min     1Q Median     3Q    Max 
## -1.752 -1.134 -0.499  1.296  2.282 
## 
## Coefficients:
##             Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept) 33.49945    3.62256  9.2475 0.0002485 ***
## wt          -2.81846    0.96619 -2.9171 0.0331257 *  
## cyl         -0.92319    0.54639 -1.6896 0.1518998    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## R-Squared: 0.8274
## Root Mean Squared Error: 1.422</code></pre></div>
<p>Spark machine learning supports a wide array of algorithms and feature transformations and as illustrated above it’s easy to chain these functions together with dplyr pipelines. To learn more see the <a href="mllib.html">machine learning</a> section.</p>
</div>
<div id="reading-and-writing-data" class="section level2">
<h2 class="hasAnchor">
<a href="#reading-and-writing-data" class="anchor"></a>Reading and Writing Data</h2>
<p>You can read and write data in CSV, JSON, and Parquet formats. Data can be stored in HDFS, S3, or on the local filesystem of cluster nodes.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">temp_csv &lt;-<span class="st"> </span><span class="kw">tempfile</span>(<span class="dt">fileext =</span> <span class="st">".csv"</span>)
temp_parquet &lt;-<span class="st"> </span><span class="kw">tempfile</span>(<span class="dt">fileext =</span> <span class="st">".parquet"</span>)
temp_json &lt;-<span class="st"> </span><span class="kw">tempfile</span>(<span class="dt">fileext =</span> <span class="st">".json"</span>)

<span class="kw"><a href="reference/spark_write_csv.html">spark_write_csv</a></span>(iris_tbl, temp_csv)
iris_csv_tbl &lt;-<span class="st"> </span><span class="kw"><a href="reference/spark_read_csv.html">spark_read_csv</a></span>(sc, <span class="st">"iris_csv"</span>, temp_csv)

<span class="kw"><a href="reference/spark_write_parquet.html">spark_write_parquet</a></span>(iris_tbl, temp_parquet)
iris_parquet_tbl &lt;-<span class="st"> </span><span class="kw"><a href="reference/spark_read_parquet.html">spark_read_parquet</a></span>(sc, <span class="st">"iris_parquet"</span>, temp_parquet)

<span class="kw"><a href="reference/spark_write_json.html">spark_write_json</a></span>(iris_tbl, temp_json)
iris_json_tbl &lt;-<span class="st"> </span><span class="kw"><a href="reference/spark_read_json.html">spark_read_json</a></span>(sc, <span class="st">"iris_json"</span>, temp_json)

<span class="kw">src_tbls</span>(sc)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode R"><code class="sourceCode r">## [1] "batting"      "flights"      "iris"         "iris_csv"    
## [5] "iris_json"    "iris_parquet" "mtcars"</code></pre></div>
</div>
<div id="distributed-r" class="section level2">
<h2 class="hasAnchor">
<a href="#distributed-r" class="anchor"></a>Distributed R</h2>
<p>You can execute arbitrary r code across your cluster using <code>spark_apply</code>. For example, we can apply <code>rgamma</code> over <code>iris</code> as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="reference/spark_apply.html">spark_apply</a></span>(iris_tbl, <span class="cf">function</span>(data) {
  data[<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>] <span class="op">+</span><span class="st"> </span><span class="kw">rgamma</span>(<span class="dv">1</span>,<span class="dv">2</span>)
})</code></pre></div>
<div class="sourceCode"><pre class="sourceCode R"><code class="sourceCode r">## # Source:   table&lt;sparklyr_tmp_16c9a2c40da6d&gt; [?? x 4]
## # Database: spark_connection
##    Sepal_Length Sepal_Width Petal_Length Petal_Width
##           &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;
##  1     7.988345    6.388345     4.288345    3.088345
##  2     7.788345    5.888345     4.288345    3.088345
##  3     7.588345    6.088345     4.188345    3.088345
##  4     7.488345    5.988345     4.388345    3.088345
##  5     7.888345    6.488345     4.288345    3.088345
##  6     8.288345    6.788345     4.588345    3.288345
##  7     7.488345    6.288345     4.288345    3.188345
##  8     7.888345    6.288345     4.388345    3.088345
##  9     7.288345    5.788345     4.288345    3.088345
## 10     7.788345    5.988345     4.388345    2.988345
## # ... with 140 more rows</code></pre></div>
<p>You can also group by columns to perform an operation over each group of rows and make use of any package within the closure:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="reference/spark_apply.html">spark_apply</a></span>(
  iris_tbl,
  <span class="cf">function</span>(e) broom<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/broom/topics/tidy">tidy</a></span>(<span class="kw">lm</span>(Petal_Width <span class="op">~</span><span class="st"> </span>Petal_Length, e)),
  <span class="dt">names =</span> <span class="kw">c</span>(<span class="st">"term"</span>, <span class="st">"estimate"</span>, <span class="st">"std.error"</span>, <span class="st">"statistic"</span>, <span class="st">"p.value"</span>),
  <span class="dt">group_by =</span> <span class="st">"Species"</span>
)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode R"><code class="sourceCode r">## # Source:   table&lt;sparklyr_tmp_16c9a477a7eb8&gt; [?? x 6]
## # Database: spark_connection
##      Species         term    estimate  std.error  statistic      p.value
##        &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;
## 1 versicolor  (Intercept) -0.08428835 0.16070140 -0.5245029 6.023428e-01
## 2 versicolor Petal_Length  0.33105360 0.03750041  8.8279995 1.271916e-11
## 3  virginica  (Intercept)  1.13603130 0.37936622  2.9945505 4.336312e-03
## 4  virginica Petal_Length  0.16029696 0.06800119  2.3572668 2.253577e-02
## 5     setosa  (Intercept) -0.04822033 0.12164115 -0.3964146 6.935561e-01
## 6     setosa Petal_Length  0.20124509 0.08263253  2.4354220 1.863892e-02</code></pre></div>
</div>
<div id="extensions" class="section level2">
<h2 class="hasAnchor">
<a href="#extensions" class="anchor"></a>Extensions</h2>
<p>The facilities used internally by sparklyr for its dplyr and machine learning interfaces are available to extension packages. Since Spark is a general purpose cluster computing system there are many potential applications for extensions (e.g. interfaces to custom machine learning pipelines, interfaces to 3rd party Spark packages, etc.).</p>
<p>Here’s a simple example that wraps a Spark text file line counting function with an R function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># write a CSV </span>
tempfile &lt;-<span class="st"> </span><span class="kw">tempfile</span>(<span class="dt">fileext =</span> <span class="st">".csv"</span>)
<span class="kw">write.csv</span>(nycflights13<span class="op">::</span>flights, tempfile, <span class="dt">row.names =</span> <span class="ot">FALSE</span>, <span class="dt">na =</span> <span class="st">""</span>)

<span class="co"># define an R interface to Spark line counting</span>
count_lines &lt;-<span class="st"> </span><span class="cf">function</span>(sc, path) {
  <span class="kw"><a href="reference/spark-api.html">spark_context</a></span>(sc) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">invoke</span>(<span class="st">"textFile"</span>, path, 1L) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">      </span><span class="kw">invoke</span>(<span class="st">"count"</span>)
}

<span class="co"># call spark to count the lines of the CSV</span>
<span class="kw">count_lines</span>(sc, tempfile)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode R"><code class="sourceCode r">## [1] 336777</code></pre></div>
<p>To learn more about creating extensions see the <a href="http://spark.rstudio.com/extensions.html">Extensions</a> section of the sparklyr website.</p>
</div>
<div id="table-utilities" class="section level2">
<h2 class="hasAnchor">
<a href="#table-utilities" class="anchor"></a>Table Utilities</h2>
<p>You can cache a table into memory with:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="reference/tbl_cache.html">tbl_cache</a></span>(sc, <span class="st">"batting"</span>)</code></pre></div>
<p>and unload from memory using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="reference/tbl_uncache.html">tbl_uncache</a></span>(sc, <span class="st">"batting"</span>)</code></pre></div>
</div>
<div id="connection-utilities" class="section level2">
<h2 class="hasAnchor">
<a href="#connection-utilities" class="anchor"></a>Connection Utilities</h2>
<p>You can view the Spark web console using the <code>spark_web</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="reference/spark_web.html">spark_web</a></span>(sc)</code></pre></div>
<p>You can show the log using the <code>spark_log</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="reference/spark_log.html">spark_log</a></span>(sc, <span class="dt">n =</span> <span class="dv">10</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode R"><code class="sourceCode r">## 17/07/27 23:10:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 87 (/var/folders/fz/v6wfsg2x1fb1rw4f6r0x4jwm0000gn/T//RtmpJO9ujN/file16c9a23decc5.csv MapPartitionsRDD[340] at textFile at NativeMethodAccessorImpl.java:0)
## 17/07/27 23:10:34 INFO TaskSchedulerImpl: Adding task set 87.0 with 1 tasks
## 17/07/27 23:10:34 INFO TaskSetManager: Starting task 0.0 in stage 87.0 (TID 148, localhost, executor driver, partition 0, PROCESS_LOCAL, 6009 bytes)
## 17/07/27 23:10:34 INFO Executor: Running task 0.0 in stage 87.0 (TID 148)
## 17/07/27 23:10:34 INFO HadoopRDD: Input split: file:/var/folders/fz/v6wfsg2x1fb1rw4f6r0x4jwm0000gn/T/RtmpJO9ujN/file16c9a23decc5.csv:0+33313106
## 17/07/27 23:10:34 INFO Executor: Finished task 0.0 in stage 87.0 (TID 148). 1123 bytes result sent to driver
## 17/07/27 23:10:34 INFO TaskSetManager: Finished task 0.0 in stage 87.0 (TID 148) in 129 ms on localhost (executor driver) (1/1)
## 17/07/27 23:10:34 INFO TaskSchedulerImpl: Removed TaskSet 87.0, whose tasks have all completed, from pool 
## 17/07/27 23:10:34 INFO DAGScheduler: ResultStage 87 (count at NativeMethodAccessorImpl.java:0) finished in 0.130 s
## 17/07/27 23:10:34 INFO DAGScheduler: Job 55 finished: count at NativeMethodAccessorImpl.java:0, took 0.132388 s</code></pre></div>
<p>Finally, we disconnect from Spark:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="reference/spark-connections.html">spark_disconnect</a></span>(sc)</code></pre></div>
</div>
<div id="rstudio-ide" class="section level2">
<h2 class="hasAnchor">
<a href="#rstudio-ide" class="anchor"></a>RStudio IDE</h2>
<p>The latest RStudio <a href="https://www.rstudio.com/products/rstudio/download/preview/">Preview Release</a> of the RStudio IDE includes integrated support for Spark and the sparklyr package, including tools for:</p>
<ul>
<li>Creating and managing Spark connections</li>
<li>Browsing the tables and columns of Spark DataFrames</li>
<li>Previewing the first 1,000 rows of Spark DataFrames</li>
</ul>
<p>Once you’ve installed the sparklyr package, you should find a new <strong>Spark</strong> pane within the IDE. This pane includes a <strong>New Connection</strong> dialog which can be used to make connections to local or remote Spark instances:</p>
<p><img src="tools/readme/spark-connect.png" class="screenshot" width="389/"></p>
<p>Once you’ve connected to Spark you’ll be able to browse the tables contained within the Spark cluster and preview Spark DataFrames using the standard RStudio data viewer:</p>
<p><img src="tools/readme/spark-dataview.png" class="screenshot" width="639/"></p>
<p>You can also connect to Spark through <a href="http://livy.io">Livy</a> through a new connection dialog:</p>
<p><img src="tools/readme/spark-connect-livy.png" class="screenshot" width="389/"></p>
<p>The RStudio IDE features for sparklyr are available now as part of the <a href="https://www.rstudio.com/products/rstudio/download/preview/">RStudio Preview Release</a>.</p>
</div>
<div id="using-h2o" class="section level2">
<h2 class="hasAnchor">
<a href="#using-h2o" class="anchor"></a>Using H2O</h2>
<p><a href="https://cran.r-project.org/package=rsparkling">rsparkling</a> is a CRAN package from <a href="http://h2o.ai">H2O</a> that extends <a href="http://spark.rstudio.com">sparklyr</a> to provide an interface into <a href="https://github.com/h2oai/sparkling-water">Sparkling Water</a>. For instance, the following example installs, configures and runs <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/glm.html">h2o.glm</a>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">options</span>(<span class="dt">rsparkling.sparklingwater.version =</span> <span class="st">"2.1.0"</span>)

<span class="kw">library</span>(rsparkling)
<span class="kw">library</span>(sparklyr)
<span class="kw">library</span>(dplyr)
<span class="kw">library</span>(h2o)

sc &lt;-<span class="st"> </span><span class="kw"><a href="reference/spark-connections.html">spark_connect</a></span>(<span class="dt">master =</span> <span class="st">"local"</span>, <span class="dt">version =</span> <span class="st">"2.1.0"</span>)
mtcars_tbl &lt;-<span class="st"> </span><span class="kw">copy_to</span>(sc, mtcars, <span class="st">"mtcars"</span>)

mtcars_h2o &lt;-<span class="st"> </span><span class="kw">as_h2o_frame</span>(sc, mtcars_tbl, <span class="dt">strict_version_check =</span> <span class="ot">FALSE</span>)

mtcars_glm &lt;-<span class="st"> </span><span class="kw">h2o.glm</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="st">"wt"</span>, <span class="st">"cyl"</span>), 
                      <span class="dt">y =</span> <span class="st">"mpg"</span>,
                      <span class="dt">training_frame =</span> mtcars_h2o,
                      <span class="dt">lambda_search =</span> <span class="ot">TRUE</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mtcars_glm</code></pre></div>
<div class="sourceCode"><pre class="sourceCode R"><code class="sourceCode r">## Model Details:
## ==============
## 
## H2ORegressionModel: glm
## Model ID:  GLM_model_R_1501222258016_1 
## GLM Model: summary
##     family     link                              regularization
## 1 gaussian identity Elastic Net (alpha = 0.5, lambda = 0.1013 )
##                                                                lambda_search
## 1 nlambda = 100, lambda.max = 10.132, lambda.min = 0.1013, lambda.1se = -1.0
##   number_of_predictors_total number_of_active_predictors
## 1                          2                           2
##   number_of_iterations                                training_frame
## 1                    0 frame_rdd_29_94c9913e0af6e119b3b413e51dad46b6
## 
## Coefficients: glm coefficients
##       names coefficients standardized_coefficients
## 1 Intercept    38.941654                 20.090625
## 2       cyl    -1.468783                 -2.623132
## 3        wt    -3.034558                 -2.969186
## 
## H2ORegressionMetrics: glm
## ** Reported on training data. **
## 
## MSE:  6.017684
## RMSE:  2.453097
## MAE:  1.940985
## RMSLE:  0.1114801
## Mean Residual Deviance :  6.017684
## R^2 :  0.8289895
## Null Deviance :1126.047
## Null D.o.F. :31
## Residual Deviance :192.5659
## Residual D.o.F. :29
## AIC :156.2425</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="reference/spark-connections.html">spark_disconnect</a></span>(sc)</code></pre></div>
</div>
<div id="connecting-through-livy" class="section level2">
<h2 class="hasAnchor">
<a href="#connecting-through-livy" class="anchor"></a>Connecting through Livy</h2>
<p><a href="https://github.com/cloudera/livy">Livy</a> enables remote connections to Apache Spark clusters. Connecting to Spark clusters through Livy is <strong>under experimental development</strong> in <code>sparklyr</code>. Please post any feedback or questions as a GitHub issue as needed.</p>
<p>Before connecting to Livy, you will need the connection information to an existing service running Livy. Otherwise, to test <code>livy</code> in your local environment, you can install it and run it locally as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="reference/livy_install.html">livy_install</a></span>()</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="reference/livy_service.html">livy_service_start</a></span>()</code></pre></div>
<p>To connect, use the Livy service address as <code>master</code> and <code>method = "livy"</code> in <code>spark_connect</code>. Once connection completes, use <code>sparklyr</code> as usual, for instance:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sc &lt;-<span class="st"> </span><span class="kw"><a href="reference/spark-connections.html">spark_connect</a></span>(<span class="dt">master =</span> <span class="st">"http://localhost:8998"</span>, <span class="dt">method =</span> <span class="st">"livy"</span>)
<span class="kw">copy_to</span>(sc, iris)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode R"><code class="sourceCode r">## # Source:   table&lt;iris&gt; [?? x 5]
## # Database: spark_connection
##    Sepal_Length Sepal_Width Petal_Length Petal_Width Species
##           &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;   &lt;chr&gt;
##  1          5.1         3.5          1.4         0.2  setosa
##  2          4.9         3.0          1.4         0.2  setosa
##  3          4.7         3.2          1.3         0.2  setosa
##  4          4.6         3.1          1.5         0.2  setosa
##  5          5.0         3.6          1.4         0.2  setosa
##  6          5.4         3.9          1.7         0.4  setosa
##  7          4.6         3.4          1.4         0.3  setosa
##  8          5.0         3.4          1.5         0.2  setosa
##  9          4.4         2.9          1.4         0.2  setosa
## 10          4.9         3.1          1.5         0.1  setosa
## # ... with 140 more rows</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="reference/spark-connections.html">spark_disconnect</a></span>(sc)</code></pre></div>
<p>Once you are done using <code>livy</code> locally, you should stop this service with:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="reference/livy_service.html">livy_service_stop</a></span>()</code></pre></div>
<p>To connect to remote <code>livy</code> clusters that support basic authentication connect as:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">config &lt;-<span class="st"> </span><span class="kw">livy_config_auth</span>(<span class="st">"&lt;username&gt;"</span>, <span class="st">"&lt;password"</span><span class="op">&gt;</span>)
sc &lt;-<span class="st"> </span><span class="kw"><a href="reference/spark-connections.html">spark_connect</a></span>(<span class="dt">master =</span> <span class="st">"&lt;address&gt;"</span>, <span class="dt">method =</span> <span class="st">"livy"</span>, <span class="dt">config =</span> config)
<span class="kw"><a href="reference/spark-connections.html">spark_disconnect</a></span>(sc)</code></pre></div>
</div>
</div>
  </div>

  <div class="col-md-3" id="sidebar">
    <h2>Links</h2>
<ul class="list-unstyled">
<li>Download from CRAN at <br><a href="https://cran.r-project.org/package=sparklyr">https://​cran.r-project.org/​package=sparklyr</a>
</li>
<li>Report a bug at <br><a href="https://github.com/rstudio/sparklyr/issues">https://​github.com/​rstudio/​sparklyr/​issues</a>
</li>
</ul>
<h2>License</h2>
<p>Apache License 2.0 | file <a href="LICENSE.html">LICENSE</a></p>
<h2>Developers</h2>
<ul class="list-unstyled">
<li>Javier Luraschi <br><small class="roles"> Author, maintainer </small> </li>
<li>Kevin Ushey <br><small class="roles"> Author </small> </li>
<li>JJ Allaire <br><small class="roles"> Author </small> </li>
<li> The Apache Software Foundation <br><small class="roles"> Author, copyright holder </small> </li>
<li><a href="authors.html">All authors...</a></li>
</ul>
<h2>Dev status</h2>
<ul class="list-unstyled">
<li><a href="https://travis-ci.org/rstudio/sparklyr"><img src="https://travis-ci.org/rstudio/sparklyr.svg?branch=master" alt="Build Status"></a></li>
<li><a href="https://cran.r-project.org/package=sparklyr"><img src="https://www.r-pkg.org/badges/version/sparklyr" alt="CRAN_Status_Badge"></a></li>
<li><a href="https://gitter.im/rstudio/sparklyr?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge"><img src="https://badges.gitter.im/rstudio/sparklyr.svg" alt="Join the chat at https://gitter.im/rstudio/sparklyr"></a></li>
</ul>
</div>
</div>


      <footer><div class="copyright">
  <p>Developed by Javier Luraschi, Kevin Ushey, JJ Allaire,  The Apache Software Foundation.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
